#!/usr/bin/env python3
"""
Project File Analyzer
ØªØ­Ù„ÛŒÙ„ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ TikTrue Platform
"""

import os
import ast
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
import hashlib

class FileInfo:
    def __init__(self, path: str):
        self.path = path
        self.name = os.path.basename(path)
        self.size = os.path.getsize(path) if os.path.exists(path) else 0
        self.extension = os.path.splitext(path)[1]
        self.imports: Set[str] = set()
        self.functions: List[str] = []
        self.classes: List[str] = []
        self.has_main = False
        self.has_docstring = False
        self.is_test = path.startswith('test_') or '/test' in path
        self.is_demo = path.startswith('demo_') or '/demo' in path
        self.hash = self._calculate_hash()
        
    def _calculate_hash(self) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ hash ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§"""
        try:
            with open(self.path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return ""

class ProjectAnalyzer:
    def __init__(self, root_path: str = "."):
        self.root_path = Path(root_path)
        self.files: Dict[str, FileInfo] = {}
        self.dependencies: Dict[str, Set[str]] = defaultdict(set)
        self.reverse_dependencies: Dict[str, Set[str]] = defaultdict(set)
        self.duplicates: List[Tuple[str, str]] = []
        self.orphaned_files: List[str] = []
        self.categories = {
            'core': [],
            'workers': [],
            'interfaces': [],
            'network': [],
            'security': [],
            'models': [],
            'config': [],
            'tests': [],
            'build': [],
            'docs': [],
            'utils': [],
            'unknown': []
        }
        
    def analyze_project(self):
        """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡"""
        print("ğŸ” Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ù¾Ø±ÙˆÚ˜Ù‡...")
        
        # 1. Ø§Ø³Ú©Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        self._scan_files()
        
        # 2. ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
        self._analyze_dependencies()
        
        # 3. Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        self._categorize_files()
        
        # 4. ØªØ´Ø®ÛŒØµ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        self._find_duplicates()
        
        # 5. ØªØ´Ø®ÛŒØµ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ orphaned
        self._find_orphaned_files()
        
        print("âœ… ØªØ­Ù„ÛŒÙ„ Ù¾Ø±ÙˆÚ˜Ù‡ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        
    def _scan_files(self):
        """Ø§Ø³Ú©Ù† ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡"""
        print("ğŸ“ Ø§Ø³Ú©Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...")
        
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ†
        ignore_patterns = {
            '__pycache__', '.git', '.venv', 'node_modules', 
            '.pytest_cache', 'dist', 'build', '.vscode',
            'logs', 'sessions', 'temp', 'test_storage'
        }
        
        ignore_extensions = {'.pyc', '.pyo', '.log', '.tmp', '.bak'}
        
        for file_path in self.root_path.rglob('*'):
            if file_path.is_file():
                # Ú†Ú© Ú©Ø±Ø¯Ù† ignore patterns
                if any(pattern in str(file_path) for pattern in ignore_patterns):
                    continue
                    
                if file_path.suffix in ignore_extensions:
                    continue
                    
                relative_path = str(file_path.relative_to(self.root_path))
                file_info = FileInfo(relative_path)
                
                # ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ†
                if file_path.suffix == '.py':
                    self._analyze_python_file(file_info, file_path)
                
                self.files[relative_path] = file_info
                
        print(f"ğŸ“Š {len(self.files)} ÙØ§ÛŒÙ„ Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
        
    def _analyze_python_file(self, file_info: FileInfo, file_path: Path):
        """ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„ Ù¾Ø§ÛŒØªÙˆÙ†"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† AST
            tree = ast.parse(content)
            
            # Ø¨Ø±Ø±Ø³ÛŒ docstring
            if (tree.body and isinstance(tree.body[0], ast.Expr) and 
                isinstance(tree.body[0].value, ast.Constant) and 
                isinstance(tree.body[0].value.value, str)):
                file_info.has_docstring = True
                
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ imports, functions, classes
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        file_info.imports.add(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        file_info.imports.add(node.module)
                elif isinstance(node, ast.FunctionDef):
                    file_info.functions.append(node.name)
                    if node.name == 'main':
                        file_info.has_main = True
                elif isinstance(node, ast.ClassDef):
                    file_info.classes.append(node.name)
                    
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ {file_path}: {e}")
            
    def _analyze_dependencies(self):
        """ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§"""
        print("ğŸ”— ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§...")
        
        for file_path, file_info in self.files.items():
            for import_name in file_info.imports:
                # ØªØ¨Ø¯ÛŒÙ„ import Ø¨Ù‡ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ù…Ø­ØªÙ…Ù„
                potential_files = [
                    f"{import_name}.py",
                    f"{import_name}/__init__.py",
                    f"{import_name.replace('.', '/')}.py"
                ]
                
                for potential_file in potential_files:
                    if potential_file in self.files:
                        self.dependencies[file_path].add(potential_file)
                        self.reverse_dependencies[potential_file].add(file_path)
                        break
                        
    def _categorize_files(self):
        """Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… Ùˆ Ù…Ø­ØªÙˆØ§"""
        print("ğŸ“‚ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...")
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        patterns = {
            'core': [
                'model_node.py', 'network_manager.py', 'config_manager.py',
                'protocol_spec.py', 'main_app.py', 'service_runner.py'
            ],
            'workers': [
                'worker_lib.py', 'scheduler_lib.py', 'homf_lib.py',
                'dynamic_profiler.py', 'static_profiler.py',
                'paged_kv_cache_lib.py', 'sequential_gpu_worker_lib.py'
            ],
            'interfaces': [
                'chatbot_interface.py', 'chat_interface.py',
                'enhanced_chat_interface.py', 'session_ui.py',
                'network_dashboard.py', 'first_run_wizard.py'
            ],
            'network': [
                'websocket_server.py', 'unified_websocket_server.py',
                'enhanced_websocket_handler.py', 'network_discovery.py',
                'api_client.py', 'backend_api_client.py'
            ],
            'security': [
                'security.license_validator.py', 'license_storage.py',
                'security.auth_manager.py', 'security.crypto_layer.py',
                'security.hardware_fingerprint.py', 'access_control.py',
                'key_manager.py'
            ],
            'models': [
                'models.model_downloader.py', 'models.model_encryption.py',
                'models.model_verification.py', 'model_selector.py',
                'model_node_license_integration.py'
            ],
            'config': [
                'network_config.json', 'portable_config.json',
                'performance_profile.json'
            ],
            'build': [
                'Build-Installer.ps1', 'build_installer_complete.py',
                'build_installer.bat', 'installer.nsi',
                'validate_installer_build.py'
            ],
            'docs': [
                'README_SETUP.md', 'PRODUCTION_READY.md',
                'Data_Full_Project_V1.md', 'About_MDI_Mainproject.txt',
                'KEY_MANAGEMENT_IMPLEMENTATION_SUMMARY.md',
                'ENHANCED_WEBSOCKET_INTEGRATION.md'
            ],
            'utils': [
                'setup_validator.py', 'serialization_utils.py',
                'custom_logging.py', 'monitoring_system.py'
            ]
        }
        
        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        for file_path in self.files:
            categorized = False
            file_name = os.path.basename(file_path)
            
            # ØªØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
            if file_name.startswith('test_') or file_name.startswith('demo_'):
                self.categories['tests'].append(file_path)
                categorized = True
            else:
                # Ø³Ø§ÛŒØ± Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§
                for category, file_patterns in patterns.items():
                    if file_name in file_patterns:
                        self.categories[category].append(file_path)
                        categorized = True
                        break
                        
            if not categorized:
                self.categories['unknown'].append(file_path)
                
    def _find_duplicates(self):
        """ØªØ´Ø®ÛŒØµ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ"""
        print("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ...")
        
        hash_to_files = defaultdict(list)
        
        for file_path, file_info in self.files.items():
            if file_info.hash and file_info.size > 0:
                hash_to_files[file_info.hash].append(file_path)
                
        for file_hash, file_list in hash_to_files.items():
            if len(file_list) > 1:
                for i in range(len(file_list)):
                    for j in range(i + 1, len(file_list)):
                        self.duplicates.append((file_list[i], file_list[j]))
                        
    def _find_orphaned_files(self):
        """ØªØ´Ø®ÛŒØµ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ orphaned (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡)"""
        print("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ orphaned...")
        
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ ÙˆØ§Ø¨Ø³ØªÙ‡ Ù†ÛŒØ³Øª
        for file_path in self.files:
            if (file_path not in self.reverse_dependencies and 
                not self.files[file_path].has_main and
                not self.files[file_path].is_test and
                not self.files[file_path].is_demo and
                file_path.endswith('.py')):
                self.orphaned_files.append(file_path)
                
    def generate_report(self) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„"""
        report = []
        report.append("=" * 60)
        report.append("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ù¾Ø±ÙˆÚ˜Ù‡ TikTrue Platform")
        report.append("=" * 60)
        
        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        report.append(f"\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ:")
        report.append(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {len(self.files)}")
        report.append(f"   â€¢ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ†: {len([f for f in self.files if f.endswith('.py')])}")
        report.append(f"   â€¢ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON: {len([f for f in self.files if f.endswith('.json')])}")
        report.append(f"   â€¢ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len([f for f in self.files if f.endswith('.md')])}")
        
        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        report.append(f"\nğŸ“‚ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§:")
        for category, files in self.categories.items():
            if files:
                report.append(f"   â€¢ {category}: {len(files)} ÙØ§ÛŒÙ„")
                for file_path in sorted(files)[:5]:  # Ù†Ù…Ø§ÛŒØ´ 5 ÙØ§ÛŒÙ„ Ø§ÙˆÙ„
                    report.append(f"     - {file_path}")
                if len(files) > 5:
                    report.append(f"     ... Ùˆ {len(files) - 5} ÙØ§ÛŒÙ„ Ø¯ÛŒÚ¯Ø±")
                    
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        if self.duplicates:
            report.append(f"\nğŸ”„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ ({len(self.duplicates)} Ø¬ÙØª):")
            for file1, file2 in self.duplicates[:10]:
                report.append(f"   â€¢ {file1} â‰ˆ {file2}")
                
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ orphaned
        if self.orphaned_files:
            report.append(f"\nğŸï¸  ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ orphaned ({len(self.orphaned_files)} ÙØ§ÛŒÙ„):")
            for file_path in self.orphaned_files[:10]:
                report.append(f"   â€¢ {file_path}")
                
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ø³ØªÙ†Ø¯Ø§Øª
        undocumented = [f for f, info in self.files.items() 
                       if f.endswith('.py') and not info.has_docstring]
        if undocumented:
            report.append(f"\nğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† docstring ({len(undocumented)} ÙØ§ÛŒÙ„):")
            for file_path in undocumented[:10]:
                report.append(f"   â€¢ {file_path}")
                
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª
        report.append(f"\nğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª:")
        report.append(f"   â€¢ Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø²Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ø³ØªÙ‡")
        report.append(f"   â€¢ Ø­Ø°Ù ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ")
        report.append(f"   â€¢ Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ orphaned")
        report.append(f"   â€¢ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† docstring Ø¨Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ†")
        
        return "\n".join(report)
        
    def save_detailed_report(self, filename: str = "project_analysis_report.json"):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ Ø¯Ø± ÙØ±Ù…Øª JSON"""
        detailed_report = {
            "summary": {
                "total_files": len(self.files),
                "python_files": len([f for f in self.files if f.endswith('.py')]),
                "json_files": len([f for f in self.files if f.endswith('.json')]),
                "markdown_files": len([f for f in self.files if f.endswith('.md')])
            },
            "categories": {k: v for k, v in self.categories.items() if v},
            "duplicates": self.duplicates,
            "orphaned_files": self.orphaned_files,
            "undocumented_files": [f for f, info in self.files.items() 
                                 if f.endswith('.py') and not info.has_docstring],
            "dependencies": {k: list(v) for k, v in self.dependencies.items()},
            "file_details": {
                path: {
                    "size": info.size,
                    "extension": info.extension,
                    "imports": list(info.imports),
                    "functions": info.functions,
                    "classes": info.classes,
                    "has_main": info.has_main,
                    "has_docstring": info.has_docstring,
                    "is_test": info.is_test,
                    "is_demo": info.is_demo
                }
                for path, info in self.files.items()
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ Ø¯Ø± {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    print("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ù¾Ø±ÙˆÚ˜Ù‡ TikTrue Platform")
    
    analyzer = ProjectAnalyzer()
    analyzer.analyze_project()
    
    # Ù†Ù…Ø§ÛŒØ´ Ú¯Ø²Ø§Ø±Ø´
    report = analyzer.generate_report()
    print(report)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ
    analyzer.save_detailed_report()
    
    print("\nâœ… ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯!")

if __name__ == "__main__":
    main()