'''
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¹Ø§Ø¯ÛŒ
python chatbot_interface.py --tokenizer-path ./llama3_1_tokenizer

# Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÙØ§Ø±Ø´ÛŒ
python chatbot_interface.py --tokenizer-path ./llama3_1_tokenizer --first-node-port 8701 --max-new-tokens 3 --bot-name "AI Assistant"

'''

import asyncio
import json
import numpy as np
import argparse
import logging
import sys
from pathlib import Path
import time
import websockets
from typing import Dict, Any, Optional, List
import colorama

try:
    from transformers import AutoTokenizer
except ImportError:
    logging.error("transformers not found. Install with: pip install transformers")
    sys.exit(1)

# Initialize colorama for colored terminal output
colorama.init(autoreset=True)

# --- Path Setup ---
# Use ConfigManager for proper path management
try:
    from core.config_manager import get_config_manager
    config_manager = get_config_manager()
except ImportError:
    # Fallback for standalone usage
    sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# --- Serialization Fallback ---
# Using the proven serialization logic from your original client.
try:
    from utils.serialization_utils import tensor_to_json_serializable, json_serializable_to_tensor
except ImportError:
    logging.warning("serialization_utils.py not found. Using local fallback for tensor serialization.")
    import base64
    
    def tensor_to_json_serializable(tensor: np.ndarray) -> dict:
        """ØªØ¨Ø¯ÛŒÙ„ tensor Ø¨Ù‡ ÙØ±Ù…Øª JSON Ù‚Ø§Ø¨Ù„ Ø§Ø±Ø³Ø§Ù„"""
        return {
            "_tensor_": True,
            "dtype": str(tensor.dtype),
            "shape": list(tensor.shape),
            "data_b64": base64.b64encode(tensor.tobytes()).decode("utf-8")
        }
    
    def json_serializable_to_tensor(data: dict) -> np.ndarray:
        """ØªØ¨Ø¯ÛŒÙ„ JSON Ø¨Ù‡ tensor"""
        if not isinstance(data, dict) or not data.get("_tensor_"):
            return data
        dt, sh, b = np.dtype(data["dtype"]), tuple(data["shape"]), base64.b64decode(data["data_b64"])
        return np.frombuffer(b, dtype=dt).reshape(sh)

# --- Main Chatbot Class ---

class ChatbotInterface:
    """Ø±Ø§Ø¨Ø· Ú†Øªâ€ŒØ¨Ø§Øª Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³Ø±ÙˆØ± inference ØªÙˆØ²ÛŒØ¹ÛŒ"""

    def __init__(self, args: argparse.Namespace):
        self.args = args
        self.tokenizer = self._load_tokenizer()
        self.chat_history: List[Dict[str, str]] = []
        self.server_uri = f"ws://{args.first_node_host}:{args.first_node_port}"
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¶Ø§ÙÛŒ
        self.max_retries = 3
        self.retry_delay = 1.0
        
    
        
        print(f"{colorama.Fore.CYAN}ğŸ¤– Chatbot initialized successfully!{colorama.Style.RESET_ALL}")
        print(f"{colorama.Fore.YELLOW}ğŸ“¡ Server: {self.server_uri}{colorama.Style.RESET_ALL}")

    def _load_tokenizer(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ tokenizer"""
        print(f"{colorama.Style.BRIGHT}ğŸ“¥ Loading tokenizer from {self.args.tokenizer_path}...{colorama.Style.RESET_ALL}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(self.args.tokenizer_path)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            print(f"{colorama.Fore.GREEN}{colorama.Style.BRIGHT}âœ… Tokenizer loaded successfully.{colorama.Style.RESET_ALL}")
            return tokenizer
        except Exception as e:
            print(f"{colorama.Fore.RED}{colorama.Style.BRIGHT}âŒ Fatal: Could not load tokenizer. Error: {e}{colorama.Style.RESET_ALL}")
            sys.exit(1)

    def _format_prompt_with_history(self, user_prompt: str) -> str:
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ prompt Ø¨Ø§ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡"""
        if not self.chat_history:
            return f"user: {user_prompt}\nassistant:"
        
        history_str = ""
        for turn in self.chat_history:
            history_str += f"user: {turn['user']}\nassistant: {turn['assistant']}\n"
        
        return history_str + f"user: {user_prompt}\nassistant:"

    def _create_position_ids(self, input_ids: np.ndarray, past_length: int = 0) -> np.ndarray:
        """Ø§ÛŒØ¬Ø§Ø¯ position_ids Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„"""
        seq_length = input_ids.shape[1]
        position_ids = np.arange(past_length, past_length + seq_length, dtype=np.int64)
        return position_ids.reshape(1, seq_length)

    
    async def _send_request_with_retry(self, payload: dict) -> Optional[dict]:
        """Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯"""
        for attempt in range(self.max_retries):
            try:
                async with websockets.connect(
                    self.server_uri, 
                    max_size=None, 
                    ping_interval=180,
                    ping_timeout=600,
                    close_timeout=1200
                ) as websocket:
                    # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
                    await websocket.send(json.dumps(payload))
                    
                    # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø®
                    response_data = await asyncio.wait_for(websocket.recv(), timeout=1200.0)
                    response = json.loads(response_data)
                    
                    # ØªØ¨Ø¯ÛŒÙ„ tensor Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù¾Ø§Ø³Ø®
                    if response.get("status") == "success":
                        # ØªØ¨Ø¯ÛŒÙ„ tensors Ø¯Ø± output_tensors
                        if "output_tensors" in response:
                            for name, tensor in response["output_tensors"].items():
                                response["output_tensors"][name] = json_serializable_to_tensor(tensor)
                        
                        # ØªØ¨Ø¯ÛŒÙ„ tensors Ø¯Ø± outputs (Ø¬Ø§ÛŒÛŒ Ú©Ù‡ logits Ù†Ù‡Ø§ÛŒÛŒ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù‡)
                        if "outputs" in response:
                            for name, tensor in response["outputs"].items():
                                if isinstance(tensor, dict) and tensor.get("_tensor_"):
                                    response["outputs"][name] = json_serializable_to_tensor(tensor)
                    
                    return response
                    
            except asyncio.TimeoutError:
                print(f"\n{colorama.Fore.RED}â° Timeout waiting for response from server (attempt {attempt + 1}/{self.max_retries}){colorama.Style.RESET_ALL}")
                
            except websockets.exceptions.ConnectionClosed as e:
                print(f"\n{colorama.Fore.RED}ğŸ”Œ Connection closed (attempt {attempt + 1}/{self.max_retries}). Is the server running? Details: {e}{colorama.Style.RESET_ALL}")
                
            except Exception as e:
                print(f"\n{colorama.Fore.RED}âŒ Communication error (attempt {attempt + 1}/{self.max_retries}): {e}{colorama.Style.RESET_ALL}")
            
            # ØªØ£Ø®ÛŒØ± Ù‚Ø¨Ù„ Ø§Ø² ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯
            if attempt < self.max_retries - 1:
                print(f"{colorama.Fore.YELLOW}ğŸ”„ Retrying in {self.retry_delay} seconds...{colorama.Style.RESET_ALL}")
                await asyncio.sleep(self.retry_delay)
        
        print(f"{colorama.Fore.RED}ğŸ’¥ All retry attempts failed!{colorama.Style.RESET_ALL}")
        return None

    async def _handle_generation_loop(self, user_prompt: str):
        """Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ ØªÙˆÙ„ÛŒØ¯ ØªÙˆÚ©Ù† Ø¨Ù‡ ØªÙˆÚ©Ù† Ø¨Ø§ KV cache Ú©Ø§Ù…Ù„"""
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ prompt Ú©Ø§Ù…Ù„
        full_prompt = self._format_prompt_with_history(user_prompt)
        
        # ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ
        inputs = self.tokenizer(
            full_prompt, 
            return_tensors="np", 
            padding=False, 
            truncation=True, 
            max_length=128
        )
        input_ids = inputs["input_ids"]
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª session
        session_id = f"chat_{int(time.time())}"
        generated_tokens = []
        
        print(f"{colorama.Fore.BLUE}{colorama.Style.BRIGHT}{self.args.bot_name}: {colorama.Style.RESET_ALL}", end="", flush=True)

        last_decoded_text = ""
        generation_start_time = time.time()

        # Ø­Ù„Ù‚Ù‡ ØªÙˆÙ„ÛŒØ¯
        for step in range(self.args.max_new_tokens):
            try:
                # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ step
                if step == 0:
                    # Ø§ÙˆÙ„ÛŒÙ† step: ØªÙ…Ø§Ù… prompt Ø±Ø§ Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    current_input_ids = input_ids
                    current_attention_mask = np.ones_like(input_ids, dtype=np.int64)
                    current_position_ids = self._create_position_ids(input_ids)
                    
                    
                else:
                    # step Ù‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ: ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† ØªÙˆÚ©Ù† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡
                    last_token = generated_tokens[-1]
                    current_input_ids = np.array([[last_token]], dtype=np.int64)
                    
                    # attention_mask Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¯Ù†Ø¨Ø§Ù„Ù‡
                    if step == 0:
                        current_attention_mask = np.ones_like(input_ids, dtype=np.int64)
                    else:
                        # Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ ÙÙ‚Ø· ÛŒÚ© ØªÙˆÚ©Ù†
                        current_attention_mask = np.ones((1, 1), dtype=np.int64)
                    
                    # position_ids Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù† Ø¬Ø¯ÛŒØ¯
                    current_position = input_ids.shape[1] + len(generated_tokens) - 1
                    current_position_ids = np.array([[current_position]], dtype=np.int64)
                    
                  
                # Ø³Ø§Ø®Øª payload Ø¨Ø§ KV cache
                input_tensors = {
                "input_ids": tensor_to_json_serializable(current_input_ids),
                "attention_mask": tensor_to_json_serializable(current_attention_mask),
                "position_ids": tensor_to_json_serializable(current_position_ids)
                }               
             

                payload = {
                    "session_id": session_id,
                    "step": step,
                    "target_block_id": self.args.initial_target_block,
                    "input_tensors": input_tensors
                }

                # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ payload Ø¨Ø±Ø§ÛŒ debug
                if step == 0:
                    print(f"\n{colorama.Fore.YELLOW}ğŸ”§ Debug - First step payload keys: {list(input_tensors.keys())}{colorama.Style.RESET_ALL}")

                # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
                response = await self._send_request_with_retry(payload)
                
                if not response or response.get("status") != "success":
                    error_msg = response.get('message', 'No response') if response else 'No response'
                    print(f"\n{colorama.Fore.RED}ğŸ’¥ Generation failed at step {step}: {error_msg}{colorama.Style.RESET_ALL}")
                    
                    # Debug: Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ response
                    if response:
                        print(f"{colorama.Fore.YELLOW}ğŸ”§ Full response keys: {list(response.keys())}{colorama.Style.RESET_ALL}")
                        if 'outputs' in response:
                            print(f"{colorama.Fore.YELLOW}ğŸ”§ Response outputs: {response['outputs']}{colorama.Style.RESET_ALL}")
                        if 'output_tensors' in response:
                            print(f"{colorama.Fore.YELLOW}ğŸ”§ Response output_tensors: {list(response['output_tensors'].keys())}{colorama.Style.RESET_ALL}")
                        if 'successful_blocks' in response:
                            print(f"{colorama.Fore.YELLOW}ğŸ”§ Successful blocks: {response['successful_blocks']}{colorama.Style.RESET_ALL}")
                        if 'failed_blocks' in response:
                            print(f"{colorama.Fore.YELLOW}ğŸ”§ Failed blocks: {response['failed_blocks']}{colorama.Style.RESET_ALL}")
                    break
                
                # Ø§Ú¯Ø± pipeline Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯Ù‡ ÙˆÙ„ÛŒ ÙÙ‚Ø· block_1 Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡
                successful_blocks = response.get('successful_blocks', [])
                if len(successful_blocks) == 1 and successful_blocks[0] == 'block_1':
                    print(f"\n{colorama.Fore.RED}ğŸš¨ PIPELINE INCOMPLETE: Only block_1 executed!{colorama.Style.RESET_ALL}")
                    print(f"  This means the pipeline stopped after the first block.")
                    print(f"  Check the server logs for errors in block_2 input preparation.")
                    print(f"  Common causes:")
                    print(f"    - Missing required inputs for block_2")
                    print(f"    - Incorrect tensor shapes from block_1")
                    print(f"    - Input mapping errors between blocks")
                    
                    # Ù†Ù…Ø§ÛŒØ´ failed_blocks Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡
                    failed_blocks = response.get('failed_blocks', [])
                    if failed_blocks:
                        print(f"  Failed blocks: {failed_blocks}")
                    
                    # Ù†Ù…Ø§ÛŒØ´ execution_times Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨ÛŒØ´ØªØ±
                    exec_times = response.get('execution_times', {})
                    if exec_times:
                        print(f"  Execution times: {exec_times}")
                    
                    break

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ logits - Ø§ÙˆÙ„ Ø§Ø² outputs Ø¨Ø¹Ø¯ Ø§Ø² output_tensors
                outputs = response.get("outputs", {})
                logits = None
                
                # Debug: Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ response structure (ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§)
                if response.get("status") != "success" or len(response.get('successful_blocks', [])) != 33:
                    print(f"\n{colorama.Fore.CYAN}ğŸ”§ Response structure:{colorama.Style.RESET_ALL}")
                    print(f"  Status: {response.get('status')}")
                    print(f"  Keys: {list(response.keys())}")
                    print(f"  Successful blocks: {response.get('successful_blocks', [])}")
                    print(f"  Total successful blocks: {len(response.get('successful_blocks', []))}")
                
                # Ø¨Ø±Ø±Ø³ÛŒ propagating_tensors (Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ logits ØªÙˆØ´ Ù‡Ø³Øª)
                if 'propagating_tensors' in response:
                    prop_tensors = response['propagating_tensors']
                    print(f"  Propagating tensors keys: {list(prop_tensors.keys()) if isinstance(prop_tensors, dict) else type(prop_tensors)}")
                    
                    # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ logits Ø¯Ø± propagating_tensors
                    if isinstance(prop_tensors, dict):
                        for key, value in prop_tensors.items():
                            if 'logits' in key.lower() or (hasattr(value, 'shape') and len(value.shape) == 3 and value.shape[-1] > 30000):
                                print(f"  ğŸ¯ Potential logits found in propagating_tensors['{key}']: shape={getattr(value, 'shape', 'unknown')}")
                
                # â— Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ block_33 Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ ÛŒØ§ Ù†Ù‡
                successful_blocks = response.get('successful_blocks', [])
                if 'block_33' not in successful_blocks:
                    print(f"\n{colorama.Fore.RED}ğŸš¨ CRITICAL: Block 33 (logits generator) was NOT executed!{colorama.Style.RESET_ALL}")
                    print(f"  Only {len(successful_blocks)} blocks executed: {successful_blocks}")
                    print(f"  Expected: 33 blocks including block_33")
                    
                    # Ù†Ù…Ø§ÛŒØ´ execution times Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù„Ø§Ú© Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡
                    if 'execution_times' in response:
                        exec_times = response['execution_times']
                        last_block = max(exec_times.keys()) if exec_times else 'none'
                        print(f"  Last executed block: {last_block}")
                        print(f"  Total blocks in pipeline: {len(exec_times)}")
                
                if 'outputs' in response:
                    print(f"  Outputs type: {type(response['outputs'])}")
                    print(f"  Outputs content: {response['outputs']}")
                if 'output_tensors' in response:
                    print(f"  Output tensors keys: {list(response['output_tensors'].keys())}")
                
                # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ logits
                outputs = response.get("outputs", {})
                logits = None
                
                # Ø±ÙˆØ´ 1: Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² outputs
                if isinstance(outputs, dict) and "logits" in outputs:
                    logits = outputs["logits"]
                    print(f"\n{colorama.Fore.GREEN}âœ… Found logits in 'outputs': {logits.shape}{colorama.Style.RESET_ALL}")
                
                # Ø±ÙˆØ´ 2: Ø§Ø² output_tensors
                elif "output_tensors" in response and "logits" in response["output_tensors"]:
                    logits = response["output_tensors"]["logits"]
                    print(f"\n{colorama.Fore.GREEN}âœ… Found logits in 'output_tensors': {logits.shape}{colorama.Style.RESET_ALL}")
                
                # Ø±ÙˆØ´ 3: Ø§Ø² propagating_tensors
                elif "propagating_tensors" in response:
                    prop_tensors = response["propagating_tensors"]
                    if isinstance(prop_tensors, dict):
                        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
                        if "logits" in prop_tensors:
                            logits = prop_tensors["logits"]
                            # ØªØ¨Ø¯ÛŒÙ„ tensor Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ù‡
                            if isinstance(logits, dict) and logits.get("_tensor_"):
                                logits = json_serializable_to_tensor(logits)
                            print(f"\n{colorama.Fore.GREEN}âœ… Found logits in 'propagating_tensors': {logits.shape}{colorama.Style.RESET_ALL}")
                        
                        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ tensor Ø¨Ø§ shape Ù…Ù†Ø§Ø³Ø¨
                        else:
                            for key, value in prop_tensors.items():
                                # ØªØ¨Ø¯ÛŒÙ„ tensor Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ù‡
                                if isinstance(value, dict) and value.get("_tensor_"):
                                    value = json_serializable_to_tensor(value)
                                
                                if hasattr(value, 'shape') and len(value.shape) == 3 and value.shape[-1] > 30000:
                                    logits = value
                                    print(f"\n{colorama.Fore.GREEN}âœ… Found logits in 'propagating_tensors[{key}]': {value.shape}{colorama.Style.RESET_ALL}")
                                    break
                
                # Ø±ÙˆØ´ 4: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ø± Ù‡Ù…Ù‡ Ø¬Ø§
                else:
                    all_sources = {
                        **outputs,
                        **response.get("output_tensors", {}),
                        **response.get("propagating_tensors", {})
                    }
                    print(f"\n{colorama.Fore.YELLOW}ğŸ”§ Searching in all sources: {list(all_sources.keys())}{colorama.Style.RESET_ALL}")
                    
                    for key, value in all_sources.items():
                        # ØªØ¨Ø¯ÛŒÙ„ tensor Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ù‡
                        if isinstance(value, dict) and value.get("_tensor_"):
                            value = json_serializable_to_tensor(value)
                        
                        if hasattr(value, 'shape') and len(value.shape) == 3 and value.shape[-1] > 30000:
                            logits = value
                            print(f"\n{colorama.Fore.GREEN}âœ… Found logits in '{key}': {value.shape}{colorama.Style.RESET_ALL}")
                            break
                
                if logits is None:
                    print(f"\n{colorama.Fore.RED}âŒ Error: 'logits' not found in server response.{colorama.Style.RESET_ALL}")
                    break
                
                # Ø§Ù†ØªØ®Ø§Ø¨ ØªÙˆÚ©Ù† Ø¨Ø¹Ø¯ÛŒ (Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ greedy)
                next_token_id = np.argmax(logits[0, -1, :]).item()
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø§ÛŒØ§Ù† Ø¯Ù†Ø¨Ø§Ù„Ù‡
                if next_token_id == self.tokenizer.eos_token_id:
                    print(f"\n{colorama.Fore.GREEN}ğŸ Generation completed (EOS token reached){colorama.Style.RESET_ALL}")
                    break
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÙˆÚ©Ù† Ø¬Ø¯ÛŒØ¯
                generated_tokens.append(next_token_id)
                
                # Ù†Ù…Ø§ÛŒØ´ ØªØ¯Ø±ÛŒØ¬ÛŒ Ù…ØªÙ†
                current_full_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=False)
                newly_decoded_text = current_full_text[len(last_decoded_text):]
                
                print(newly_decoded_text, end="", flush=True)
                last_decoded_text = current_full_text
                
            except Exception as e:
                print(f"\n{colorama.Fore.RED}ğŸ’¥ Error at step {step}: {e}{colorama.Style.RESET_ALL}")
                import traceback
                traceback.print_exc()
                break

        print()  # Ø®Ø· Ø¬Ø¯ÛŒØ¯
        
        # Ø¢Ù…Ø§Ø± ØªÙˆÙ„ÛŒØ¯
        generation_time = time.time() - generation_start_time
        tokens_per_second = len(generated_tokens) / generation_time if generation_time > 0 else 0
        
        print(f"{colorama.Fore.CYAN}ğŸ“Š Generated {len(generated_tokens)} tokens in {generation_time:.2f}s ({tokens_per_second:.1f} tokens/s){colorama.Style.RESET_ALL}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
        if generated_tokens:
            full_response_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
            self.chat_history.append({"user": user_prompt, "assistant": full_response_text})

    def _show_help(self):
        """Ù†Ù…Ø§ÛŒØ´ Ø±Ø§Ù‡Ù†Ù…Ø§"""
        print(f"\n{colorama.Style.BRIGHT}ğŸ“š Available Commands:{colorama.Style.RESET_ALL}")
        print(f"{colorama.Fore.GREEN}  clear{colorama.Style.RESET_ALL}     - Clear conversation history")
        print(f"{colorama.Fore.GREEN}  history{colorama.Style.RESET_ALL}   - Show conversation history")
        print(f"{colorama.Fore.GREEN}  stats{colorama.Style.RESET_ALL}     - Show session statistics")
        print(f"{colorama.Fore.GREEN}  help{colorama.Style.RESET_ALL}      - Show this help")
        print(f"{colorama.Fore.GREEN}  exit/quit{colorama.Style.RESET_ALL} - End session")
        print()

    def _show_history(self):
        """Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡"""
        if not self.chat_history:
            print(f"{colorama.Fore.YELLOW}ğŸ“ No conversation history yet.{colorama.Style.RESET_ALL}")
            return
        
        print(f"\n{colorama.Style.BRIGHT}ğŸ“š Conversation History:{colorama.Style.RESET_ALL}")
        print("=" * 60)
        
        for i, turn in enumerate(self.chat_history, 1):
            print(f"{colorama.Fore.GREEN}[{i}] You:{colorama.Style.RESET_ALL} {turn['user']}")
            print(f"{colorama.Fore.BLUE}[{i}] {self.args.bot_name}:{colorama.Style.RESET_ALL} {turn['assistant']}")
            print("-" * 40)
        print()

    def _show_stats(self):
        """Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± session"""
        total_turns = len(self.chat_history)
        total_user_tokens = sum(len(self.tokenizer.encode(turn['user'])) for turn in self.chat_history)
        total_bot_tokens = sum(len(self.tokenizer.encode(turn['assistant'])) for turn in self.chat_history)
        
        print(f"\n{colorama.Style.BRIGHT}ğŸ“Š Session Statistics:{colorama.Style.RESET_ALL}")
        print(f"  ğŸ’¬ Total turns: {total_turns}")
        print(f"  ğŸ‘¤ User tokens: {total_user_tokens}")
        print(f"  ğŸ¤– Bot tokens: {total_bot_tokens}")
        print(f"  ğŸ“¡ Server: {self.server_uri}")
        print(f"  ğŸ¯ Max tokens per generation: {self.args.max_new_tokens}")
        print(f"  ğŸ§  Model layers: {self.num_layers}")
        print()

    async def run(self):
        """Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±"""
        print("\n" + "=" * 60)
        print(f"{colorama.Style.BRIGHT}      ğŸš€ Advanced Interactive Chat Client{colorama.Style.RESET_ALL}")
        print("=" * 60)
        print(f" {colorama.Fore.CYAN}ğŸ’¡ Type 'help' for available commands{colorama.Style.RESET_ALL}")
        print("=" * 60 + "\n")

        while True:
            try:
                prompt = input(f"{colorama.Fore.GREEN}{colorama.Style.BRIGHT}You: {colorama.Style.RESET_ALL}")
                
                if not prompt.strip():
                    continue
                    
                prompt_lower = prompt.lower().strip()
                
                # Ø¯Ø³ØªÙˆØ±Ø§Øª Ø®Ø§Øµ
                if prompt_lower in ["exit", "quit"]:
                    break
                elif prompt_lower == "clear":
                    self.chat_history = []
                    print(f"\n{colorama.Fore.YELLOW}ğŸ§¹ Conversation history cleared!{colorama.Style.RESET_ALL}\n")
                    continue
                elif prompt_lower == "history":
                    self._show_history()
                    continue
                elif prompt_lower == "stats":
                    self._show_stats()
                    continue
                elif prompt_lower == "help":
                    self._show_help()
                    continue
                
                # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
                await self._handle_generation_loop(prompt)
                print()  # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ù…Ú©Ø§Ù„Ù…Ø§Øª

            except (EOFError, KeyboardInterrupt):
                print(f"\n{colorama.Fore.YELLOW}ğŸ‘‹ Interrupted by user{colorama.Style.RESET_ALL}")
                break
            except Exception as e:
                print(f"\n{colorama.Fore.RED}ğŸ’¥ Unexpected error: {e}{colorama.Style.RESET_ALL}")
                continue
        
        print(f"\n{colorama.Style.BRIGHT}ğŸ‘‹ Thank you for using the chatbot! Goodbye!{colorama.Style.RESET_ALL}")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    parser = argparse.ArgumentParser(
        description="ğŸ¤– Advanced Chatbot Interface for Distributed Inference",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python chatbot_interface.py --tokenizer-path ./llama3_1_tokenizer
  python chatbot_interface.py --tokenizer-path ./models/tokenizer --max-new-tokens 50
  python chatbot_interface.py --first-node-host 192.168.1.100 --first-node-port 8701
        """
    )
    
    parser.add_argument("--first-node-host", 
                       default="localhost", 
                       help="Hostname of the first inference node")
    
    parser.add_argument("--first-node-port", 
                       type=int, 
                       default=8702, 
                       help="Port of the first inference node")
    
    parser.add_argument("--initial-target-block", 
                       default="block_1", 
                       help="Initial target block ID for pipeline")
    
    parser.add_argument("--tokenizer-path", 
                       help="Path to the tokenizer directory (auto-detected if not provided)")
    
    parser.add_argument("--model", 
                       help="Model ID to use (e.g., llama3_1_8b_fp16, mistral_7b_int4)")
    
    parser.add_argument("--max-new-tokens", 
                       type=int, 
                       default=100, 
                       help="Maximum number of tokens to generate per response")
    
    parser.add_argument("--bot-name", 
                       default="AI Assistant", 
                       help="Display name for the chatbot")
    
    parser.add_argument("--list-models", 
                       action="store_true", 
                       help="List available models and exit")
    
    args = parser.parse_args()
    
    # Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ùˆ Ø®Ø±ÙˆØ¬
    if args.list_models:
        try:
            from model_selector import ModelSelector
            selector = ModelSelector()
            selector.list_models()
        except Exception as e:
            print(f"âŒ Error listing models: {e}")
        sys.exit(0)
    
    # ØªÙ†Ø¸ÛŒÙ… tokenizer path Ø®ÙˆØ¯Ú©Ø§Ø±
    if not args.tokenizer_path:
        try:
            from core.config_manager import ConfigManager
            
            # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ØŒ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            if args.model:
                config_manager = ConfigManager(model_id=args.model)
            else:
                config_manager = ConfigManager()
            
            tokenizer_path = config_manager.get_tokenizer_path()
            if tokenizer_path:
                args.tokenizer_path = str(tokenizer_path)
                print(f"{colorama.Fore.GREEN}âœ… Auto-detected tokenizer: {args.tokenizer_path}{colorama.Style.RESET_ALL}")
            else:
                print(f"{colorama.Fore.RED}âŒ No tokenizer found. Please specify --tokenizer-path{colorama.Style.RESET_ALL}")
                sys.exit(1)
                
        except Exception as e:
            print(f"{colorama.Fore.RED}âŒ Error auto-detecting tokenizer: {e}{colorama.Style.RESET_ALL}")
            if not args.tokenizer_path:
                print("Please specify --tokenizer-path manually")
                sys.exit(1)
    
    # Ù†Ù…Ø§ÛŒØ´ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    print(f"{colorama.Style.BRIGHT}ğŸ”§ Configuration:{colorama.Style.RESET_ALL}")
    print(f"  ğŸ“ Server: {args.first_node_host}:{args.first_node_port}")
    print(f"  ğŸ­ Tokenizer: {args.tokenizer_path}")
    print(f"  ğŸ¯ Max tokens: {args.max_new_tokens}")
    print(f"  ğŸ¤– Bot name: {args.bot_name}")
    if args.model:
        print(f"  ğŸ§  Model: {args.model}")
    
    try:
        interface = ChatbotInterface(args)
        asyncio.run(interface.run())
    except KeyboardInterrupt:
        print(f"\n{colorama.Fore.YELLOW}ğŸ‘‹ Goodbye!{colorama.Style.RESET_ALL}")
    except Exception as e:
        print(f"{colorama.Fore.RED}ğŸ’¥ Fatal error: {e}{colorama.Style.RESET_ALL}")
        sys.exit(1)


if __name__ == "__main__":
    main()